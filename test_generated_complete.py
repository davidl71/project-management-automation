# Test for suggest_test_cases
\ndef suggest_test_cases(\n    target_file: Optional[str] = None,\n    test_framework: str = \"pytest\",\n    min_confidence: float = 0.7,\n    output_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"\n    Suggest test cases based on code analysis.\n\n    Args:\n        target_file: File to analyze (optional - analyzes all Python files if not provided)\n        test_framework: Framework for suggestions (default: pytest)\n        min_confidence: Minimum confidence threshold (default: 0.7)\n        output_path: Path for suggestions output (optional)\n\n    Returns:\n        Dictionary with suggested test cases\n    \"\"\"\n    import time\n    start_time = time.time()\n\n    try:\n        from ..utils import find_project_root\n\n        project_root = find_project_root()\n        suggestions = []\n\n        if target_file:\n            # Analyze specific file\n            file_path = Path(target_file)\n            if not file_path.is_absolute():\n                file_path = project_root / file_path\n\n            if file_path.exists() and file_path.suffix == \".py\":\n                suggestions.extend(_analyze_file(file_path, test_framework, min_confidence))\n        else:\n            # Analyze all Python files in project\n            for py_file in project_root.rglob(\"*.py\"):\n                # Skip test files, venv, and hidden directories\n                if \"test\" in str(py_file).lower() or \"venv\" in str(py_file) or py_file.name.startswith(\".\"):\n                    continue\n                if py_file.parent.name.startswith(\".\"):\n                    continue\n\n                file_suggestions = _analyze_file(py_file, test_framework, min_confidence)\n                if file_suggestions:\n                    suggestions.extend(file_suggestions)\n\n        # Filter by confidence\n        filtered_suggestions = [\n            s for s in suggestions\n            if s.get(\"confidence\", 0.0) >= min_confidence\n        ]\n\n        # Sort by confidence (highest first)\n        filtered_suggestions.sort(key=lambda x: x.get(\"confidence\", 0.0), reverse=True)\n\n        result = {\n            \"suggestions_count\": len(filtered_suggestions),\n            \"total_analyzed\": len(suggestions),\n            \"min_confidence\": min_confidence,\n            \"framework\": test_framework,\n            \"suggestions\": filtered_suggestions[:50],  # Top 50\n        }\n\n        # Save to file if requested\n        if output_path:\n            output_file = Path(output_path)\n            if not output_file.is_absolute():\n                output_file = project_root / output_file\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n\n            with open(output_file, \"w\") as f:\n                json.dump(result, f, indent=2)\n\n            result[\"output_file\"] = str(output_file)\n\n        duration = time.time() - start_time\n        log_automation_execution(\"suggest_test_cases\", duration, True)\n\n        return format_success_response(result)\n\n    except Exception as e:\n        duration = time.time() - start_time\n        log_automation_execution(\"suggest_test_cases\", duration, False, e)\n        logger.error(f\"Error suggesting test cases: {e}\", exc_info=True)\n\n        error_response = format_error_response(e, ErrorCode.AUTOMATION_ERROR)\n        return error_response\n

# Test for generate_test_code
\ndef generate_test_code(\n    target_file: Optional[str] = None,\n    test_framework: str = \"pytest\",\n    use_mlx: bool = True,\n    use_coreml: bool = True,  # Try Core ML first (NPU acceleration)\n    coreml_model_path: Optional[str] = None,  # Path to Core ML model\n    model: str = \"mlx-community/Phi-3.5-mini-instruct-4bit\",  # MLX fallback model\n    max_tokens: int = 512,\n    temperature: float = 0.3,\n    compute_units: str = \"all\",  # Core ML compute units (all, cpu_and_ane, etc.)\n    output_path: Optional[str] = None,\n) -> dict[str, Any]:\n    \"\"\"\n    Generate actual test code using AI models (Core ML NPU or MLX GPU).\n\n    Uses AI models to generate complete, runnable test code from function signatures.\n    This is an enhancement over suggest_test_cases() which only provides templates.\n\n    Priority order:\n    1. Core ML model (if available) - Uses Neural Engine (NPU) for fastest inference\n    2. MLX model - Uses Metal GPU acceleration\n    3. Template fallback - Basic test templates\n\n    Args:\n        target_file: File to analyze (required for code generation)\n        test_framework: Framework for test generation (default: pytest)\n        use_coreml: Try Core ML first for NPU acceleration (default: True)\n        coreml_model_path: Path to Core ML model file (.mlpackage or .mlmodel)\n        use_mlx: Use MLX as fallback if Core ML unavailable (default: True)\n        model: MLX model to use (default: Phi-3.5-mini-instruct-4bit)\n        max_tokens: Maximum tokens to generate (default: 512)\n        temperature: Generation temperature, lower = more deterministic (default: 0.3)\n        compute_units: Core ML compute units (all, cpu_and_ane, cpu_and_gpu, cpu_only)\n        output_path: Path to save generated test code (optional)\n\n    Returns:\n        Dictionary with generated test code\n    \"\"\"\n    import time\n    start_time = time.time()\n\n    try:\n        from ..utils import find_project_root\n\n        if not target_file:\n            return format_error_response(\n                \"target_file parameter required for test code generation\",\n                ErrorCode.AUTOMATION_ERROR\n            )\n\n        project_root = find_project_root()\n        file_path = Path(target_file)\n        if not file_path.is_absolute():\n            file_path = project_root / file_path\n\n        if not file_path.exists() or file_path.suffix != \".py\":\n            return format_error_response(\n                f\"File not found or not a Python file: {target_file}\",\n                ErrorCode.AUTOMATION_ERROR\n            )\n\n        # Extract functions from file\n        functions = _extract_functions_with_code(file_path)\n        if not functions:\n            return format_error_response(\n                f\"No functions found in {target_file}\",\n                ErrorCode.AUTOMATION_ERROR\n            )\n\n        generated_tests = []\n\n        # Generate test code for each function\n        # Priority: Core ML (NPU) > MLX (GPU) > Template\n        for func_info in functions:\n            test_code = None\n            method_used = \"template\"\n            \n            # Try Core ML first (NPU acceleration)\n            if use_coreml and coreml_model_path:\n                try:\n                    test_code = _generate_test_with_coreml(\n                        func_info=func_info,\n                        test_framework=test_framework,\n                        model_path=coreml_model_path,\n                        compute_units=compute_units,\n                        max_tokens=max_tokens,\n                    )\n                    if test_code:\n                        method_used = \"coreml_neural_engine\"\n                except Exception as e:\n                    logger.debug(f\"Core ML generation failed for {func_info['name']}: {e}\")\n            \n            # Fallback to MLX (GPU acceleration)\n            if not test_code and use_mlx:\n                try:\n                    test_code = _generate_test_with_mlx(\n                        func_info=func_info,\n                        test_framework=test_framework,\n                        model=model,\n                        max_tokens=max_tokens,\n                        temperature=temperature,\n                    )\n                    if test_code and not test_code.startswith('{\"success\": false'):  # Check for error JSON\n                        method_used = \"mlx_gpu\"\n                    else:\n                        test_code = None  # MLX returned error\n                except Exception as e:\n                    logger.debug(f\"MLX generation failed for {func_info['name']}: {e}\")\n            \n            # Final fallback to template\n            if not test_code:\n                test_code = _generate_test_template(func_info, test_framework)\n                method_used = \"template_fallback\" if (use_coreml or use_mlx) else \"template\"\n            \n            generated_tests.append({\n                \"function\": func_info[\"name\"],\n                \"test_code\": test_code,\n                \"framework\": test_framework,\n                \"method\": method_used,\n            })\n\n        # Combine all test code\n        combined_test_code = \"\\n\\n\".join([\n            f\"# Test for {test['function']}\\n{test['test_code']}\"\n            for test in generated_tests\n        ])\n\n        result = {\n            \"target_file\": str(file_path.relative_to(project_root)),\n            \"functions_tested\": len(generated_tests),\n            \"test_framework\": test_framework,\n            \"generated_tests\": generated_tests,\n            \"combined_test_code\": combined_test_code,\n            \"method\": \"coreml_neural_engine\" if (use_coreml and coreml_model_path) else (\"mlx_gpu\" if use_mlx else \"template\"),\n        }\n\n        # Save to file if requested\n        if output_path:\n            output_file = Path(output_path)\n            if not output_file.is_absolute():\n                output_file = project_root / output_file\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n\n            with open(output_file, \"w\") as f:\n                f.write(combined_test_code)\n\n            result[\"output_file\"] = str(output_file)\n\n        duration = time.time() - start_time\n        log_automation_execution(\"generate_test_code\", duration, True)\n\n        return format_success_response(result)\n\n    except Exception as e:\n        duration = time.time() - start_time\n        log_automation_execution(\"generate_test_code\", duration, False, e)\n        logger.error(f\"Error generating test code: {e}\", exc_info=True)\n\n        error_response = format_error_response(e, ErrorCode.AUTOMATION_ERROR)\n        return error_response\n

# Test for format_success_response
\n        def format_success_response(data, message=None):\n            return {\"success\": True, \"data\": data, \"timestamp\": time.time()}\n

# Test for format_error_response
\n        def format_error_response(error, error_code, include_traceback=False):\n            return {\"success\": False, \"error\": {\"code\": str(error_code), \"message\": str(error)}}\n

# Test for log_automation_execution
\n        def log_automation_execution(name, duration, success, error=None):\n            logger.info(f\"{name}: {duration:.2f}s, success={success}\")\n